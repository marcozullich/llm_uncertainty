{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzullich/LLMs_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token decrypted successfully.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login as huggingface_login\n",
    "from utils import decrypt_huggingface_token\n",
    "\n",
    "huggingface_login(token=decrypt_huggingface_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:52<00:00, 10.51s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# For larger models, consider quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #bnb_4bit_use_double_quant=True, # Often helps\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Apply quantization if desired\n",
    "    # torch_dtype=torch.bfloat16, # Or torch.float16\n",
    "    device_map=\"auto\" # Automatically distribute model layers across available devices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a cat who learns to fly. The story must include the following elements: a cat named Whiskers, a magical object, a mountain, and a transformation. Make sure the story has a clear beginning, middle, and end, and that the transformation is the climax. Also, ensure that the story is appropriate for children ages 8-12.\n",
      "\n",
      "**Title: Whiskers and the Skyward Stone**\n",
      "\n",
      "**Beginning:**  \n",
      "In the quiet village of Mewntown, nestled between rolling hills and a towering mountain called\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using the pipeline for simplicity\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Write a short story about a cat who learns to fly.\"\n",
    "generated_text = generator(prompt, max_new_tokens=100, num_return_sequences=1)\n",
    "\n",
    "print(generated_text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " tensor([0.7129, 0.8558, 0.6946, 1.0000, 1.0000], device='cuda:0'),\n",
       " tensor(0.4238, device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_with_logits = generation_output\n",
    "\n",
    "\n",
    "logits = torch.cat(generation_with_logits.scores)\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "per_token_confidence = probs.max(dim=-1).values\n",
    "\n",
    "A = torch.prod(per_token_confidence)\n",
    "\n",
    "probs, per_token_confidence, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt (token IDs): [785, 6722, 315, 9625, 374]\n",
      "Generated sequence (full token IDs): [785, 6722, 315, 9625, 374, 12095, 13, 3555, 374, 279]\n",
      "Newly generated tokens (IDs): [12095, 13, 3555, 374, 279]\n",
      "\n",
      "--- Detailed Output ---\n",
      "\n",
      "Token 1: ' Paris' (ID: 12095)\n",
      "Probability of chosen token: 0.7129\n",
      "Top 5 predictions for this step:\n",
      "  - ' Paris' (ID: 12095): 0.7129\n",
      "  - ' a' (ID: 264): 0.0855\n",
      "  - ' located' (ID: 7407): 0.0732\n",
      "  - ' in' (ID: 304): 0.0479\n",
      "  - ' __' (ID: 1304): 0.0342\n",
      "\n",
      "Token 2: '.' (ID: 13)\n",
      "Probability of chosen token: 0.8558\n",
      "Top 5 predictions for this step:\n",
      "  - '.' (ID: 13): 0.8558\n",
      "  - ',' (ID: 11): 0.0785\n",
      "  - '.\n",
      "' (ID: 624): 0.0657\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 3: ' What' (ID: 3555)\n",
      "Probability of chosen token: 0.2275\n",
      "Top 5 predictions for this step:\n",
      "  - ' The' (ID: 576): 0.6946\n",
      "  - ' What' (ID: 3555): 0.2275\n",
      "  - ' Which' (ID: 15920): 0.0349\n",
      "  - ' This' (ID: 1096): 0.0239\n",
      "  - ' Is' (ID: 2160): 0.0100\n",
      "\n",
      "Token 4: ' is' (ID: 374)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' is' (ID: 374): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 5: ' the' (ID: 279)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' the' (ID: 279): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "--- Final Structured Output ---\n",
      "[\n",
      "  {\n",
      "    \"token_id\": 12095,\n",
      "    \"decoded_token\": \" Paris\",\n",
      "    \"probability_of_chosen\": 0.712932288646698,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 12095,\n",
      "        \"decoded_token\": \" Paris\",\n",
      "        \"probability\": 0.712932288646698\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 264,\n",
      "        \"decoded_token\": \" a\",\n",
      "        \"probability\": 0.08552849292755127\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 7407,\n",
      "        \"decoded_token\": \" located\",\n",
      "        \"probability\": 0.07315640151500702\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 304,\n",
      "        \"decoded_token\": \" in\",\n",
      "        \"probability\": 0.04787019267678261\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1304,\n",
      "        \"decoded_token\": \" __\",\n",
      "        \"probability\": 0.034249477088451385\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 13,\n",
      "    \"decoded_token\": \".\",\n",
      "    \"probability_of_chosen\": 0.8557648658752441,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 13,\n",
      "        \"decoded_token\": \".\",\n",
      "        \"probability\": 0.8557648658752441\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 11,\n",
      "        \"decoded_token\": \",\",\n",
      "        \"probability\": 0.07853954285383224\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 624,\n",
      "        \"decoded_token\": \".\\n\",\n",
      "        \"probability\": 0.0656956285238266\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 3555,\n",
      "    \"decoded_token\": \" What\",\n",
      "    \"probability_of_chosen\": 0.22751830518245697,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 576,\n",
      "        \"decoded_token\": \" The\",\n",
      "        \"probability\": 0.6945779323577881\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3555,\n",
      "        \"decoded_token\": \" What\",\n",
      "        \"probability\": 0.22751830518245697\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 15920,\n",
      "        \"decoded_token\": \" Which\",\n",
      "        \"probability\": 0.03489113226532936\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1096,\n",
      "        \"decoded_token\": \" This\",\n",
      "        \"probability\": 0.02387346141040325\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2160,\n",
      "        \"decoded_token\": \" Is\",\n",
      "        \"probability\": 0.009996476583182812\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 374,\n",
      "    \"decoded_token\": \" is\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 374,\n",
      "        \"decoded_token\": \" is\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 279,\n",
      "    \"decoded_token\": \" the\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 279,\n",
      "        \"decoded_token\": \" the\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "Full generated text: The capital of France is Paris. What is the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=5, # Generate 5 new tokens\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True, # This will return the logits for each generated token\n",
    "    do_sample=True,     # Use sampling to get more varied probabilities\n",
    "    temperature=0.7,    # Lower temperature for less randomness\n",
    "    top_k=50,           # Top-k sampling\n",
    "    top_p=0.95          # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "generated_ids = generation_output.sequences[0]\n",
    "generated_scores = generation_output.scores\n",
    "\n",
    "start_index_of_new_tokens = input_ids.shape[1]\n",
    "new_generated_ids = generated_ids[start_index_of_new_tokens:]\n",
    "\n",
    "print(f\"Prompt (token IDs): {input_ids[0].tolist()}\")\n",
    "print(f\"Generated sequence (full token IDs): {generated_ids.tolist()}\")\n",
    "print(f\"Newly generated tokens (IDs): {new_generated_ids.tolist()}\")\n",
    "\n",
    "print(\"\\n--- Detailed Output ---\")\n",
    "decoded_tokens_with_softmax = []\n",
    "\n",
    "# Process each newly generated token and its corresponding scores\n",
    "for i, token_id in enumerate(new_generated_ids):\n",
    "    # Get the logits for the i-th generated token\n",
    "    # scores[i] corresponds to the logits for predicting the (i+1)th generated token\n",
    "    # (after the first i tokens were generated)\n",
    "    logits_for_current_token = generated_scores[i][0] # [0] because batch_size is 1\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(logits_for_current_token, dim=-1)\n",
    "\n",
    "    # Get the probability of the *chosen* token\n",
    "    chosen_token_prob = probabilities[token_id].item()\n",
    "\n",
    "    # Get the top N probable tokens and their probabilities for this step\n",
    "    top_k_values, top_k_indices = torch.topk(probabilities, k=5) # Get top 5\n",
    "\n",
    "    # Decode the chosen token\n",
    "    decoded_chosen_token = tokenizer.decode(token_id)\n",
    "\n",
    "    print(f\"\\nToken {i+1}: '{decoded_chosen_token}' (ID: {token_id})\")\n",
    "    print(f\"Probability of chosen token: {chosen_token_prob:.4f}\")\n",
    "    print(\"Top 5 predictions for this step:\")\n",
    "    for j in range(top_k_values.shape[0]):\n",
    "        top_prob = top_k_values[j].item()\n",
    "        top_token_id = top_k_indices[j].item()\n",
    "        top_decoded_token = tokenizer.decode(top_token_id)\n",
    "        print(f\"  - '{top_decoded_token}' (ID: {top_token_id}): {top_prob:.4f}\")\n",
    "\n",
    "    decoded_tokens_with_softmax.append({\n",
    "        'token_id': token_id.item(),\n",
    "        'decoded_token': decoded_chosen_token,\n",
    "        'probability_of_chosen': chosen_token_prob,\n",
    "        'top_predictions': [\n",
    "            {'token_id': top_k_indices[j].item(), 'decoded_token': tokenizer.decode(top_k_indices[j].item()), 'probability': top_k_values[j].item()}\n",
    "            for j in range(top_k_values.shape[0])\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Final Structured Output ---\")\n",
    "import json\n",
    "print(json.dumps(decoded_tokens_with_softmax, indent=2, ensure_ascii=False))\n",
    "\n",
    "# To get the full generated text from the token IDs\n",
    "full_decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"\\nFull generated text: {full_decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

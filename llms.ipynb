{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and Uncertainty\n",
    "\n",
    "The estimation of uncertainty with LLMs is much more complicated than regular regression or classification models.\n",
    "\n",
    "The output of a classification model is just a probability distribution over $c$ classes:\n",
    "\n",
    "$$\n",
    "\\hat{y}=[\\hat{y}_1,\\dots,\\hat{y}_c]\n",
    "$$\n",
    "\n",
    "LLMs works with sequential generation of text.\n",
    "Text is composed of tokens, which compose words, which compose sentences.\n",
    "\n",
    "We start with a prompt $p$ composed of a variable amount, say, $p$, of tokens $\\tau$:\n",
    "\n",
    "$$\n",
    "\\text{prompt} = [\\tau_1,\\dots,\\tau_p]\n",
    "$$\n",
    "\n",
    "and the model proceeds to generate up to $k$ tokens.\n",
    "\n",
    "$$\n",
    "\\hat{y} = [\\hat{\\tau}_1,\\dots,\\hat{\\tau}_k]\n",
    "$$\n",
    "\n",
    "Actually, each output token $\\tau$ is determined by a probability distribution over the full dictionary of tokens, thus resembling the usual classification behavior.\n",
    "At each generation step, say $j$, the prompt is enriched with the initial prompt, plus all the generated tokens up to $j-1$:\n",
    "\n",
    "$$\n",
    "\\text{prompt}_j = \\text{prompt} + [\\hat{\\tau}_1,\\dots,\\hat{\\tau}_{j-1}]\n",
    "$$\n",
    "\n",
    "This makes it so that the generation process can be assimilated to a sequential classification problem.\n",
    "\n",
    "The second issue is related to the fact that there is often no real \"correct\" solutions to a given LLM task: there can be several different ways a response can be answered with (and each token is gonna have a different probability level associated to it - this would lead to different uncertainties associated to answers with the same level of \"correctness\").\n",
    "Also, this makes it so that there can be confusion as to which tokens are considered as \"inconfident\" because of the fact that there are more possible answers, or which are inconfident because the model truly does not have a knowledge over the specific question or topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzullich/LLMs_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token decrypted successfully.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login as huggingface_login\n",
    "from utils import decrypt_huggingface_token\n",
    "\n",
    "huggingface_login(token=decrypt_huggingface_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "We load one of the many pre-trained models available on HuggingFace.\n",
    "There are several options, we use Qwen2.5-7B-Instruct-1M, a ChatGPT-like model with a relatively small memory footprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:01<00:00, 30.43s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" \n",
    "\n",
    "# For larger models, consider quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #bnb_4bit_use_double_quant=True, # Often helps\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Apply quantization if desired\n",
    "    # torch_dtype=torch.bfloat16, # Or torch.float16\n",
    "    device_map=\"auto\" # Automatically distribute model layers across available devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular text generation\n",
    "\n",
    "Let us first indicate how the model can be prompted for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a cat who learns to fly. In the quiet town of Willowbrook, nestled between rolling hills and whispering woods, lived a curious calico cat named Whiskers. She was no ordinary cat. With fur that shimmered in the sunlight and eyes that sparkled like stars, Whiskers was always eager for new adventures. But what set her apart from other cats was her insatiable curiosity and a peculiar fascination with the sky.\n",
      "\n",
      "One crisp autumn morning, as the leaves danced in the breeze, Whiskers found herself\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using the pipeline for simplicity\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Write a short story about a cat who learns to fly.\"\n",
    "generated_text = generator(prompt, max_new_tokens=100, num_return_sequences=1)\n",
    "\n",
    "print(generated_text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tokens\n",
    "\n",
    "The previous behavior is not really useful. We actually need the model to return the logits associated to each of the generated tokens. We need to call the generate attribute of the model to get this, along with other information on the output.\n",
    "\n",
    "In the cell below you can see an example of how we can inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt (token IDs): [3838, 374, 279, 6722, 315, 9625, 30]\n",
      "Generated sequence (full token IDs): [3838, 374, 279, 6722, 315, 9625, 30, 576, 6722, 315, 9625, 374, 12095, 13, 12095, 702, 1012, 279, 6722, 2474, 220, 20]\n",
      "Newly generated tokens (IDs): [576, 6722, 315, 9625, 374, 12095, 13, 12095, 702, 1012, 279, 6722, 2474, 220, 20]\n",
      "\n",
      "--- Detailed Output ---\n",
      "\n",
      "Token 1: ' The' (ID: 576)\n",
      "Probability of chosen token: 0.9424\n",
      "Top 5 predictions for this step:\n",
      "  - ' The' (ID: 576): 0.9424\n",
      "  - ' What' (ID: 3555): 0.0300\n",
      "  - ' (' (ID: 320): 0.0142\n",
      "  - ' -' (ID: 481): 0.0135\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 2: ' capital' (ID: 6722)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' capital' (ID: 6722): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 3: ' of' (ID: 315)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' of' (ID: 315): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 4: ' France' (ID: 9625)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' France' (ID: 9625): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 5: ' is' (ID: 374)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' is' (ID: 374): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 6: ' Paris' (ID: 12095)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' Paris' (ID: 12095): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 7: '.' (ID: 13)\n",
      "Probability of chosen token: 0.7533\n",
      "Top 5 predictions for this step:\n",
      "  - '.' (ID: 13): 0.7533\n",
      "  - '.\n",
      "\n",
      "' (ID: 382): 0.2467\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 8: ' Paris' (ID: 12095)\n",
      "Probability of chosen token: 0.4857\n",
      "Top 5 predictions for this step:\n",
      "  - ' Paris' (ID: 12095): 0.4857\n",
      "  - ' \n",
      "\n",
      "' (ID: 4710): 0.3808\n",
      "  - ' It' (ID: 1084): 0.0976\n",
      "  - ' However' (ID: 4354): 0.0191\n",
      "  - ' This' (ID: 1096): 0.0114\n",
      "\n",
      "Token 9: ' has' (ID: 702)\n",
      "Probability of chosen token: 0.5490\n",
      "Top 5 predictions for this step:\n",
      "  - ' has' (ID: 702): 0.5490\n",
      "  - ' is' (ID: 374): 0.4510\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 10: ' been' (ID: 1012)\n",
      "Probability of chosen token: 0.9241\n",
      "Top 5 predictions for this step:\n",
      "  - ' been' (ID: 1012): 0.9241\n",
      "  - ' held' (ID: 5644): 0.0759\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 11: ' the' (ID: 279)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - ' the' (ID: 279): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "Token 12: ' capital' (ID: 6722)\n",
      "Probability of chosen token: 0.5615\n",
      "Top 5 predictions for this step:\n",
      "  - ' capital' (ID: 6722): 0.5615\n",
      "  - ' country' (ID: 3146): 0.4121\n",
      "  - ' official' (ID: 3946): 0.0265\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 13: ' since' (ID: 2474)\n",
      "Probability of chosen token: 0.5129\n",
      "Top 5 predictions for this step:\n",
      "  - ' since' (ID: 2474): 0.5129\n",
      "  - ' city' (ID: 3283): 0.2936\n",
      "  - ' of' (ID: 315): 0.1935\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 14: ' ' (ID: 220)\n",
      "Probability of chosen token: 0.2285\n",
      "Top 5 predictions for this step:\n",
      "  - ' the' (ID: 279): 0.4144\n",
      "  - ' around' (ID: 2163): 0.3571\n",
      "  - ' ' (ID: 220): 0.2285\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "\n",
      "Token 15: '5' (ID: 20)\n",
      "Probability of chosen token: 1.0000\n",
      "Top 5 predictions for this step:\n",
      "  - '5' (ID: 20): 1.0000\n",
      "  - '#' (ID: 2): 0.0000\n",
      "  - '!' (ID: 0): 0.0000\n",
      "  - '$' (ID: 3): 0.0000\n",
      "  - '\"' (ID: 1): 0.0000\n",
      "\n",
      "--- Final Structured Output ---\n",
      "[\n",
      "  {\n",
      "    \"token_id\": 576,\n",
      "    \"decoded_token\": \" The\",\n",
      "    \"probability_of_chosen\": 0.9423742890357971,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 576,\n",
      "        \"decoded_token\": \" The\",\n",
      "        \"probability\": 0.9423742890357971\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3555,\n",
      "        \"decoded_token\": \" What\",\n",
      "        \"probability\": 0.029956357553601265\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 320,\n",
      "        \"decoded_token\": \" (\",\n",
      "        \"probability\": 0.014181997627019882\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 481,\n",
      "        \"decoded_token\": \" -\",\n",
      "        \"probability\": 0.013487318530678749\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 6722,\n",
      "    \"decoded_token\": \" capital\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 6722,\n",
      "        \"decoded_token\": \" capital\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 315,\n",
      "    \"decoded_token\": \" of\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 315,\n",
      "        \"decoded_token\": \" of\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 9625,\n",
      "    \"decoded_token\": \" France\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 9625,\n",
      "        \"decoded_token\": \" France\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 374,\n",
      "    \"decoded_token\": \" is\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 374,\n",
      "        \"decoded_token\": \" is\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 12095,\n",
      "    \"decoded_token\": \" Paris\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 12095,\n",
      "        \"decoded_token\": \" Paris\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 13,\n",
      "    \"decoded_token\": \".\",\n",
      "    \"probability_of_chosen\": 0.7532597184181213,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 13,\n",
      "        \"decoded_token\": \".\",\n",
      "        \"probability\": 0.7532597184181213\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 382,\n",
      "        \"decoded_token\": \".\\n\\n\",\n",
      "        \"probability\": 0.24674031138420105\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 12095,\n",
      "    \"decoded_token\": \" Paris\",\n",
      "    \"probability_of_chosen\": 0.4857160151004791,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 12095,\n",
      "        \"decoded_token\": \" Paris\",\n",
      "        \"probability\": 0.4857160151004791\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 4710,\n",
      "        \"decoded_token\": \" \\n\\n\",\n",
      "        \"probability\": 0.3807763159275055\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1084,\n",
      "        \"decoded_token\": \" It\",\n",
      "        \"probability\": 0.09757345914840698\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 4354,\n",
      "        \"decoded_token\": \" However\",\n",
      "        \"probability\": 0.019127754494547844\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1096,\n",
      "        \"decoded_token\": \" This\",\n",
      "        \"probability\": 0.011447221040725708\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 702,\n",
      "    \"decoded_token\": \" has\",\n",
      "    \"probability_of_chosen\": 0.5490022301673889,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 702,\n",
      "        \"decoded_token\": \" has\",\n",
      "        \"probability\": 0.5490022301673889\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 374,\n",
      "        \"decoded_token\": \" is\",\n",
      "        \"probability\": 0.45099779963493347\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 1012,\n",
      "    \"decoded_token\": \" been\",\n",
      "    \"probability_of_chosen\": 0.9241418242454529,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 1012,\n",
      "        \"decoded_token\": \" been\",\n",
      "        \"probability\": 0.9241418242454529\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 5644,\n",
      "        \"decoded_token\": \" held\",\n",
      "        \"probability\": 0.07585817575454712\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 279,\n",
      "    \"decoded_token\": \" the\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 279,\n",
      "        \"decoded_token\": \" the\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 6722,\n",
      "    \"decoded_token\": \" capital\",\n",
      "    \"probability_of_chosen\": 0.5614572167396545,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 6722,\n",
      "        \"decoded_token\": \" capital\",\n",
      "        \"probability\": 0.5614572167396545\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3146,\n",
      "        \"decoded_token\": \" country\",\n",
      "        \"probability\": 0.41208142042160034\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3946,\n",
      "        \"decoded_token\": \" official\",\n",
      "        \"probability\": 0.026461319997906685\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 2474,\n",
      "    \"decoded_token\": \" since\",\n",
      "    \"probability_of_chosen\": 0.512916088104248,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 2474,\n",
      "        \"decoded_token\": \" since\",\n",
      "        \"probability\": 0.512916088104248\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3283,\n",
      "        \"decoded_token\": \" city\",\n",
      "        \"probability\": 0.2935580611228943\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 315,\n",
      "        \"decoded_token\": \" of\",\n",
      "        \"probability\": 0.19352589547634125\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 220,\n",
      "    \"decoded_token\": \" \",\n",
      "    \"probability_of_chosen\": 0.22850912809371948,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 279,\n",
      "        \"decoded_token\": \" the\",\n",
      "        \"probability\": 0.41439422965049744\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2163,\n",
      "        \"decoded_token\": \" around\",\n",
      "        \"probability\": 0.3570966422557831\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 220,\n",
      "        \"decoded_token\": \" \",\n",
      "        \"probability\": 0.22850912809371948\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_id\": 20,\n",
      "    \"decoded_token\": \"5\",\n",
      "    \"probability_of_chosen\": 1.0,\n",
      "    \"top_predictions\": [\n",
      "      {\n",
      "        \"token_id\": 20,\n",
      "        \"decoded_token\": \"5\",\n",
      "        \"probability\": 1.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 2,\n",
      "        \"decoded_token\": \"#\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 0,\n",
      "        \"decoded_token\": \"!\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 3,\n",
      "        \"decoded_token\": \"$\",\n",
      "        \"probability\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"token_id\": 1,\n",
      "        \"decoded_token\": \"\\\"\",\n",
      "        \"probability\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "Full generated text: What is the capital of France? The capital of France is Paris. Paris has been the capital since 5\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=15, # Generate 5 new tokens\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True, # This will return the logits for each generated token\n",
    "    do_sample=True,     # Use sampling to get more varied probabilities\n",
    "    temperature=0.7,    # Lower temperature for less randomness\n",
    "    top_k=50,           # Top-k sampling\n",
    "    top_p=0.95          # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "generated_ids = generation_output.sequences[0]\n",
    "generated_scores = generation_output.scores\n",
    "\n",
    "start_index_of_new_tokens = input_ids.shape[1]\n",
    "new_generated_ids = generated_ids[start_index_of_new_tokens:]\n",
    "\n",
    "print(f\"Prompt (token IDs): {input_ids[0].tolist()}\")\n",
    "print(f\"Generated sequence (full token IDs): {generated_ids.tolist()}\")\n",
    "print(f\"Newly generated tokens (IDs): {new_generated_ids.tolist()}\")\n",
    "\n",
    "print(\"\\n--- Detailed Output ---\")\n",
    "decoded_tokens_with_softmax = []\n",
    "\n",
    "# Process each newly generated token and its corresponding scores\n",
    "for i, token_id in enumerate(new_generated_ids):\n",
    "    # Get the logits for the i-th generated token\n",
    "    # scores[i] corresponds to the logits for predicting the (i+1)th generated token\n",
    "    # (after the first i tokens were generated)\n",
    "    logits_for_current_token = generated_scores[i][0] # [0] because batch_size is 1\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(logits_for_current_token, dim=-1)\n",
    "\n",
    "    # Get the probability of the *chosen* token\n",
    "    chosen_token_prob = probabilities[token_id].item()\n",
    "\n",
    "    # Get the top N probable tokens and their probabilities for this step\n",
    "    top_k_values, top_k_indices = torch.topk(probabilities, k=5) # Get top 5\n",
    "\n",
    "    # Decode the chosen token\n",
    "    decoded_chosen_token = tokenizer.decode(token_id)\n",
    "\n",
    "    print(f\"\\nToken {i+1}: '{decoded_chosen_token}' (ID: {token_id})\")\n",
    "    print(f\"Probability of chosen token: {chosen_token_prob:.4f}\")\n",
    "    print(\"Top 5 predictions for this step:\")\n",
    "    for j in range(top_k_values.shape[0]):\n",
    "        top_prob = top_k_values[j].item()\n",
    "        top_token_id = top_k_indices[j].item()\n",
    "        top_decoded_token = tokenizer.decode(top_token_id)\n",
    "        print(f\"  - '{top_decoded_token}' (ID: {top_token_id}): {top_prob:.4f}\")\n",
    "\n",
    "    decoded_tokens_with_softmax.append({\n",
    "        'token_id': token_id.item(),\n",
    "        'decoded_token': decoded_chosen_token,\n",
    "        'probability_of_chosen': chosen_token_prob,\n",
    "        'top_predictions': [\n",
    "            {'token_id': top_k_indices[j].item(), 'decoded_token': tokenizer.decode(top_k_indices[j].item()), 'probability': top_k_values[j].item()}\n",
    "            for j in range(top_k_values.shape[0])\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Final Structured Output ---\")\n",
    "import json\n",
    "print(json.dumps(decoded_tokens_with_softmax, indent=2, ensure_ascii=False))\n",
    "\n",
    "# To get the full generated text from the token IDs\n",
    "full_decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"\\nFull generated text: {full_decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function for getting the logits alongside the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_logits(prompt, model, tokenizer, max_new_tokens=15):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    generated_ids = generation_output.sequences[0]\n",
    "    full_decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generation_output, full_decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbalized uncertainty\n",
    "\n",
    "The first thing we can do is to prompt the model to return the uncertainty. Here is an example of prompts we can use, obtained from the paper [On Verbalized Confidence Scores for LLMs, Yang et al., 2024](https://arxiv.org/abs/2412.14737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_ANSWER_TEXT = \"NO ANSWER\"\n",
    "PROMPT_NO_ANSWER = f\"If you cannot provide an answer, answer with `{NO_ANSWER_TEXT}`.\"\n",
    "\n",
    "PATTERN_SEP = r\"\\n(.*\\n)*?\"\n",
    "PATTERN_ANSWER = r\".+\"\n",
    "PATTERN_FLOAT = r\"\\d*\\.?\\d+\"\n",
    "\n",
    "VERBALIZED_UNCERTAINTY_PROMPT = f\"After your answer, provide a confidence score in percentage which measures how confident you are in your answer. Use the following format to respond:\\n```\\nAnswer: [Write your answer here.]\\nConfidence: [Write your confidence score here.]\\n```\\n{PROMPT_NO_ANSWER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of France?\\nAfter your answer, provide a confidence score in percentage which measures how confident you are in your answer. Use the following format to respond:\\n```\\nAnswer: [Write your answer here.]\\nConfidence: [Write your confidence score here.]\\n```\\nIf you cannot provide an answer, answer with `NO ANSWER`. Confidence score should be between 0 and 100.\\n```\\nAnswer: Paris\\nConfidence: 100\\n```'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_complete = prompt + \"\\n\" + VERBALIZED_UNCERTAINTY_PROMPT\n",
    "generator(prompt_complete, max_new_tokens=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token uncertainty\n",
    "\n",
    "Let us switch our attention to token-level uncertainty.\n",
    "Within the `uncertainty` library I have created, you will find 3 different implementations of uncertainty:\n",
    "\n",
    "1. **Naive token uncertainty (1 - product of single-token confidence)**\n",
    "\n",
    "$$\n",
    "\\text{Uncertainty}_{\\text{naive}} = 1 - \\prod_{j=1}^{k} \\max(\\tau_j)\n",
    "$$\n",
    "\n",
    "2. **Vanilla token uncertainty (1 - average of single-token confidence)**\n",
    "\n",
    "$$\n",
    "\\text{Uncertainty}_{\\text{vanilla}} = 1 - \\frac{\\sum_{j=1}^{k} \\max(\\tau_j)}{k}\n",
    "$$\n",
    "\n",
    "3. **LogTokU (from [Estimating LLM Uncertainty with Evidence, Ma et al., 2025](https://arxiv.org/abs/2502.00290))**\n",
    "\n",
    "Operates a disentangling between per-token aleatoric and epistemic uncertainty on the **logits space** of tokens.\n",
    "\n",
    "We retrieve the logits associated to the token prediction $\\tau_j$, to which we strip the negative part by applying elementwise ReLU.\n",
    "Remember that the logits form a real-valued vector of logit elements over the whole dictionary of size $V$:\n",
    "\n",
    "![](imgs/logits.png)\n",
    "\n",
    "Here, $V$ indicates the size of the dictionary.\n",
    "\n",
    "We restrict ourselves to a single one of these output logits $l^{(j)}$.\n",
    "\n",
    "- We select only the top-$\\kappa$ values: $\\alpha^{(j)} \\doteq \\text{top}\\kappa_{v\\in\\{1,\\dots,V\\}}(l^{(j)}_v)$\n",
    "- We suppress the negative coefficients by means of element-wise ReLU: $\\alpha^{(j)} \\leftarrow \\text{ReLU}(\\alpha^{(j)})$\n",
    "- We define the total evidence associated to the vector $\\alpha^{(j)}$: $\\alpha_0^{(j)} \\doteq \\sum_t\\alpha_t^{(j)} $\n",
    "\n",
    "Then we can operate the disentangling into aleatoric and epistemic uncertainty:\n",
    "\n",
    "$$\n",
    "\\text{Aleatoric}_j = - \\sum_{t=1}^{\\kappa} \\frac{\\alpha^{(j)}_t}{\\alpha_0^{(j)}}\\left(\\digamma(\\alpha_t^{(j)} + 1 ) - \\digamma(\\alpha_0^{(j)} + 1 )\\right),\n",
    "$$\n",
    "\n",
    "where $\\digamma$ indicate the [digamma function](https://en.wikipedia.org/wiki/Digamma_function).\n",
    "\n",
    "$$\n",
    "\\text{Epistemic}_j = \\frac{\\kappa}{\\sum_{t=1}^{\\kappa} (\\alpha_t^{(j)} + 1)}.\n",
    "$$\n",
    "\n",
    "The interpretation of the aleatoric and epistemic uncertainty is as follows:\n",
    "\n",
    "- An output with high aleatoric uncertainty indicates a general lack of knowledge of the model in the specific domain of the prompt\n",
    "- An output with high epistemic uncertainty indicates an undecisiveness of the model in determining the \"correct\" answer to a problem, and it could also happen because there are multiple correct options\n",
    "\n",
    "The authors propose to combine Aleatoric and Epistemic uncertainty to obtain an unreliability metric:\n",
    "\n",
    "$$\n",
    "\\text{Unreliability}_j = \\text{Epistemic}_j\\cdot\\text{Aleatoric}_j.\n",
    "$$\n",
    "\n",
    "And, for a generic response, we can compute the total reliability of the answer\n",
    "\n",
    "$$\n",
    "\\text{Unreliability} = \\frac{\\sum_{j=1}^{k} \\text{Unreliability}_j}{k},\n",
    "$$\n",
    "\n",
    "even though the authors suggest restricting the calculation to the top-$\\kappa$ most unreliable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzullich/LLMs_uncertainty/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from uncertainty import token_uncertainty_naive, token_uncertainty_vanilla, logTokU, get_token_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOKEN_UNCERTAINTY = \"Respond to the question with a short answer. If you are prompted to provide a single answer, just respond with that answer. Do not add any extra detail.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_with_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROMPT_TOKEN_UNCERTAINTY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generation_output, full_decoded_text = \u001b[43mgenerate_with_logits\u001b[49m(prompt, model, tokenizer, max_new_tokens=\u001b[32m100\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(full_decoded_text)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_with_logits' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = f\"{PROMPT_TOKEN_UNCERTAINTY}\\nWhat is the capital of France?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mU\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogTokU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_inconfident\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMs_uncertainty/uncertainty.py:41\u001b[39m, in \u001b[36mlogTokU\u001b[39m\u001b[34m(generation_with_logits, top_k_inconfident)\u001b[39m\n\u001b[32m     38\u001b[39m aleatoric_uncertainty = logTokU_aleatoric(per_token_confidence)\n\u001b[32m     40\u001b[39m reliability_scores = - epistemic_uncertainty * aleatoric_uncertainty\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m reliability_total = \u001b[32m1\u001b[39m/top_k_inconfident * \u001b[43mreliability_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k_inconfident\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlargest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.values.sum()\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reliability_total\n",
      "\u001b[31mRuntimeError\u001b[39m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "U.logTokU(generation_output, top_k_inconfident=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Plamplamping? - Answers\\nMath and Arithmetic\\nWhat is the capital of Plamplamping?\\nAsked by Wiki User\\nBe the first to answer!\\nüôè\\n01:42\\nHD\\n —Å–µ—Ä—å–µ–∑–Ω\\nAnswer\\nüôè\\n01:42\\nHD\\n-seri-–∑–Ω\\nRelated Questions\\nWhat is the capital of Plamplamping?\\nWhat is the capital of Swaziland?\\nWhat is the capital of the capital of the capital of the capital of the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of Plamplamping?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1575)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.logTokU(generation_output, top_k_inconfident=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you give me one name of president? Sure! Abraham Lincoln was the 16th president of the United States. If you need more information or another example, feel free to ask! üòä\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Could you give me one name of president?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0751)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.logTokU(generation_output, top_k_inconfident=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

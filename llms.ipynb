{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs and Uncertainty\n",
    "\n",
    "The estimation of uncertainty with LLMs is much more complicated than regular regression or classification models.\n",
    "\n",
    "The output of a classification model is just a probability distribution over $c$ classes:\n",
    "\n",
    "$$\n",
    "\\hat{y}=[\\hat{y}_1,\\dots,\\hat{y}_c]\n",
    "$$\n",
    "\n",
    "LLMs works with sequential generation of text.\n",
    "Text is composed of tokens, which compose words, which compose sentences.\n",
    "\n",
    "We start with a prompt $p$ composed of a variable amount, say, $p$, of tokens $\\tau$:\n",
    "\n",
    "$$\n",
    "\\text{prompt} = [\\tau_1,\\dots,\\tau_p]\n",
    "$$\n",
    "\n",
    "and the model proceeds to generate up to $k$ tokens.\n",
    "\n",
    "$$\n",
    "\\hat{y} = [\\hat{\\tau}_1,\\dots,\\hat{\\tau}_k]\n",
    "$$\n",
    "\n",
    "Actually, each output token $\\tau$ is determined by a probability distribution over the full dictionary of tokens, thus resembling the usual classification behavior.\n",
    "At each generation step, say $j$, the prompt is enriched with the initial prompt, plus all the generated tokens up to $j-1$:\n",
    "\n",
    "$$\n",
    "\\text{prompt}_j = \\text{prompt} + [\\hat{\\tau}_1,\\dots,\\hat{\\tau}_{j-1}]\n",
    "$$\n",
    "\n",
    "This makes it so that the generation process can be assimilated to a sequential classification problem.\n",
    "\n",
    "The second issue is related to the fact that there is often no real \"correct\" solutions to a given LLM task: there can be several different ways a response can be answered with (and each token is gonna have a different probability level associated to it - this would lead to different uncertainties associated to answers with the same level of \"correctness\").\n",
    "Also, this makes it so that there can be confusion as to which tokens are considered as \"inconfident\" because of the fact that there are more possible answers, or which are inconfident because the model truly does not have a knowledge over the specific question or topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login as huggingface_login\n",
    "from utils import decrypt_huggingface_token\n",
    "\n",
    "huggingface_login(token=decrypt_huggingface_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "We load one of the many pre-trained models available on HuggingFace.\n",
    "There are several options, we use Qwen2.5-7B-Instruct-1M, a ChatGPT-like model with a relatively small memory footprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" \n",
    "\n",
    "# For larger models, consider quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #bnb_4bit_use_double_quant=True, # Often helps\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, # Apply quantization if desired\n",
    "    # torch_dtype=torch.bfloat16, # Or torch.float16\n",
    "    device_map=\"auto\" # Automatically distribute model layers across available devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular text generation\n",
    "\n",
    "Let us first indicate how the model can be prompted for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using the pipeline for simplicity\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Write a short story about a cat who learns to fly.\"\n",
    "generated_text = generator(prompt, max_new_tokens=100, num_return_sequences=1)\n",
    "\n",
    "print(generated_text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tokens\n",
    "\n",
    "The previous behavior is not really useful. We actually need the model to return the logits associated to each of the generated tokens. We need to call the generate attribute of the model to get this, along with other information on the output.\n",
    "\n",
    "In the cell below you can see an example of how we can inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=15, # Generate 5 new tokens\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True, # This will return the logits for each generated token\n",
    "    do_sample=True,     # Use sampling to get more varied probabilities\n",
    "    temperature=0.7,    # Lower temperature for less randomness\n",
    "    top_k=50,           # Top-k sampling\n",
    "    top_p=0.95          # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "generated_ids = generation_output.sequences[0]\n",
    "generated_scores = generation_output.scores\n",
    "\n",
    "start_index_of_new_tokens = input_ids.shape[1]\n",
    "new_generated_ids = generated_ids[start_index_of_new_tokens:]\n",
    "\n",
    "print(f\"Prompt (token IDs): {input_ids[0].tolist()}\")\n",
    "print(f\"Generated sequence (full token IDs): {generated_ids.tolist()}\")\n",
    "print(f\"Newly generated tokens (IDs): {new_generated_ids.tolist()}\")\n",
    "\n",
    "print(\"\\n--- Detailed Output ---\")\n",
    "decoded_tokens_with_softmax = []\n",
    "\n",
    "# Process each newly generated token and its corresponding scores\n",
    "for i, token_id in enumerate(new_generated_ids):\n",
    "    # Get the logits for the i-th generated token\n",
    "    # scores[i] corresponds to the logits for predicting the (i+1)th generated token\n",
    "    # (after the first i tokens were generated)\n",
    "    logits_for_current_token = generated_scores[i][0] # [0] because batch_size is 1\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(logits_for_current_token, dim=-1)\n",
    "\n",
    "    # Get the probability of the *chosen* token\n",
    "    chosen_token_prob = probabilities[token_id].item()\n",
    "\n",
    "    # Get the top N probable tokens and their probabilities for this step\n",
    "    top_k_values, top_k_indices = torch.topk(probabilities, k=5) # Get top 5\n",
    "\n",
    "    # Decode the chosen token\n",
    "    decoded_chosen_token = tokenizer.decode(token_id)\n",
    "\n",
    "    print(f\"\\nToken {i+1}: '{decoded_chosen_token}' (ID: {token_id})\")\n",
    "    print(f\"Probability of chosen token: {chosen_token_prob:.4f}\")\n",
    "    print(\"Top 5 predictions for this step:\")\n",
    "    for j in range(top_k_values.shape[0]):\n",
    "        top_prob = top_k_values[j].item()\n",
    "        top_token_id = top_k_indices[j].item()\n",
    "        top_decoded_token = tokenizer.decode(top_token_id)\n",
    "        print(f\"  - '{top_decoded_token}' (ID: {top_token_id}): {top_prob:.4f}\")\n",
    "\n",
    "    decoded_tokens_with_softmax.append({\n",
    "        'token_id': token_id.item(),\n",
    "        'decoded_token': decoded_chosen_token,\n",
    "        'probability_of_chosen': chosen_token_prob,\n",
    "        'top_predictions': [\n",
    "            {'token_id': top_k_indices[j].item(), 'decoded_token': tokenizer.decode(top_k_indices[j].item()), 'probability': top_k_values[j].item()}\n",
    "            for j in range(top_k_values.shape[0])\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Final Structured Output ---\")\n",
    "import json\n",
    "print(json.dumps(decoded_tokens_with_softmax, indent=2, ensure_ascii=False))\n",
    "\n",
    "# To get the full generated text from the token IDs\n",
    "full_decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(f\"\\nFull generated text: {full_decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function for getting the logits alongside the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_logits(prompt, model, tokenizer, max_new_tokens=15):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    generated_ids = generation_output.sequences[0]\n",
    "    full_decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generation_output, full_decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbalized uncertainty\n",
    "\n",
    "The first thing we can do is to prompt the model to return the uncertainty. Here is an example of prompts we can use, obtained from the paper [On Verbalized Confidence Scores for LLMs, Yang et al., 2024](https://arxiv.org/abs/2412.14737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_ANSWER_TEXT = \"NO ANSWER\"\n",
    "PROMPT_NO_ANSWER = f\"If you cannot provide an answer, answer with `{NO_ANSWER_TEXT}`.\"\n",
    "\n",
    "PATTERN_SEP = r\"\\n(.*\\n)*?\"\n",
    "PATTERN_ANSWER = r\".+\"\n",
    "PATTERN_FLOAT = r\"\\d*\\.?\\d+\"\n",
    "\n",
    "VERBALIZED_UNCERTAINTY_PROMPT = f\"After your answer, provide a confidence score in percentage which measures how confident you are in your answer. Use the following format to respond:\\n```\\nAnswer: [Write your answer here.]\\nConfidence: [Write your confidence score here.]\\n```\\n{PROMPT_NO_ANSWER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_complete = prompt + \"\\n\" + VERBALIZED_UNCERTAINTY_PROMPT\n",
    "generator(prompt_complete, max_new_tokens=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token uncertainty\n",
    "\n",
    "Let us switch our attention to token-level uncertainty.\n",
    "Within the `uncertainty` library I have created, you will find 3 different implementations of uncertainty:\n",
    "\n",
    "1. **Naive token uncertainty (1 - product of single-token confidence)**\n",
    "\n",
    "$$\n",
    "\\text{Uncertainty}_{\\text{naive}} = 1 - \\prod_{j=1}^{k} \\max(\\tau_j)\n",
    "$$\n",
    "\n",
    "2. **Vanilla token uncertainty (1 - average of single-token confidence)**\n",
    "\n",
    "$$\n",
    "\\text{Uncertainty}_{\\text{vanilla}} = 1 - \\frac{\\sum_{j=1}^{k} \\max(\\tau_j)}{k}\n",
    "$$\n",
    "\n",
    "3. **LogTokU (from [Estimating LLM Uncertainty with Evidence, Ma et al., 2025](https://arxiv.org/abs/2502.00290))**\n",
    "\n",
    "Operates a disentangling between per-token aleatoric and epistemic uncertainty on the **logits space** of tokens.\n",
    "\n",
    "We retrieve the logits associated to the token prediction $\\tau_j$, to which we strip the negative part by applying elementwise ReLU.\n",
    "Remember that the logits form a real-valued vector of logit elements over the whole dictionary of size $V$:\n",
    "\n",
    "![](imgs/logits.png)\n",
    "\n",
    "Here, $V$ indicates the size of the dictionary.\n",
    "\n",
    "We restrict ourselves to a single one of these output logits $l^{(j)}$.\n",
    "\n",
    "- We select only the top-$\\kappa$ values: $\\alpha^{(j)} \\doteq \\text{top}\\kappa_{v\\in\\{1,\\dots,V\\}}(l^{(j)}_v)$\n",
    "- We suppress the negative coefficients by means of element-wise ReLU: $\\alpha^{(j)} \\leftarrow \\text{ReLU}(\\alpha^{(j)})$\n",
    "- We define the total evidence associated to the vector $\\alpha^{(j)}$: $\\alpha_0^{(j)} \\doteq \\sum_t\\alpha_t^{(j)} $\n",
    "\n",
    "Then we can operate the disentangling into aleatoric and epistemic uncertainty:\n",
    "\n",
    "$$\n",
    "\\text{Aleatoric}_j = - \\sum_{t=1}^{\\kappa} \\frac{\\alpha^{(j)}_t}{\\alpha_0^{(j)}}\\left(\\digamma(\\alpha_t^{(j)} + 1 ) - \\digamma(\\alpha_0^{(j)} + 1 )\\right),\n",
    "$$\n",
    "\n",
    "where $\\digamma$ indicate the [digamma function](https://en.wikipedia.org/wiki/Digamma_function).\n",
    "\n",
    "$$\n",
    "\\text{Epistemic}_j = \\frac{\\kappa}{\\sum_{t=1}^{\\kappa} (\\alpha_t^{(j)} + 1)}.\n",
    "$$\n",
    "\n",
    "The interpretation of the aleatoric and epistemic uncertainty is as follows:\n",
    "\n",
    "- An output with high aleatoric uncertainty indicates a general lack of knowledge of the model in the specific domain of the prompt\n",
    "- An output with high epistemic uncertainty indicates an undecisiveness of the model in determining the \"correct\" answer to a problem, and it could also happen because there are multiple correct options\n",
    "\n",
    "The authors propose to combine Aleatoric and Epistemic uncertainty to obtain an unreliability metric:\n",
    "\n",
    "$$\n",
    "\\text{Unreliability}_j = \\text{Epistemic}_j\\cdot\\text{Aleatoric}_j.\n",
    "$$\n",
    "\n",
    "And, for a generic response, we can compute the total reliability of the answer\n",
    "\n",
    "$$\n",
    "\\text{Unreliability} = \\frac{\\sum_{j=1}^{k} \\text{Unreliability}_j}{k},\n",
    "$$\n",
    "\n",
    "even though the authors suggest restricting the calculation to the top-$\\kappa$ most unreliable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uncertainty as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOKEN_UNCERTAINTY = \"Respond to the question with a short answer. If you are prompted to provide a single answer, just respond with that answer. Do not add any extra detail.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"{PROMPT_TOKEN_UNCERTAINTY}\\nWhat is the capital of France?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.token_uncertainty_naive(generation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.token_uncertainty_vanilla(generation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.logTokU(generation_output, top_k_inconfident=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of Plamplamping?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.logTokU(generation_output, top_k_inconfident=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Could you give me one name of president?\"\n",
    "generation_output, full_decoded_text = generate_with_logits(prompt, model, tokenizer, max_new_tokens=100)\n",
    "print(full_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U.token_uncertainty_naive(generation_output), U.token_uncertainty_vanilla(generation_output), U.logTokU(generation_output, top_k_inconfident=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
